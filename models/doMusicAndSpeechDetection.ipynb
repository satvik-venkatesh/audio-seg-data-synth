{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "doMusicAndSpeechDetection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "13dlCSNvlZ5PqBtUcTp30Iw42-Ipt7Vjp",
      "authorship_tag": "ABX9TyNar9zWbsAkSXk/T4nwwCjv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satvik-venkatesh/audio-seg-data-synth/blob/main/models/doMusicAndSpeechDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOFJwr-6na0f"
      },
      "source": [
        "import soundfile as sf\n",
        "import argparse\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ9J12x7ZHNu"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJEnwns-nduO"
      },
      "source": [
        "def smooth_output(output, min_speech=1.3, min_music=3.4, max_silence_speech=0.4, max_silence_music=0.6):\n",
        "    duration_frame = 220 / 22050\n",
        "    n_frame = output.shape[1]\n",
        "\n",
        "    start_music = -1000\n",
        "    start_speech = -1000\n",
        "\n",
        "    for i in range(n_frame):\n",
        "        if output[0, i] == 1:\n",
        "            if i - start_speech > 1:\n",
        "                if (i - start_speech) * duration_frame <= max_silence_speech:\n",
        "                    output[0, start_speech:i] = 1\n",
        "\n",
        "            start_speech = i\n",
        "\n",
        "        if output[1, i] == 1:\n",
        "            if i - start_music > 1:\n",
        "                if (i - start_music) * duration_frame <= max_silence_music:\n",
        "                    output[1, start_music:i] = 1\n",
        "\n",
        "            start_music = i\n",
        "\n",
        "    start_music = -1000\n",
        "    start_speech = -1000\n",
        "\n",
        "    for i in range(n_frame):\n",
        "        if i != n_frame - 1:\n",
        "            if output[0, i] == 0:\n",
        "                if i - start_speech > 1:\n",
        "                    if (i - start_speech) * duration_frame <= min_speech:\n",
        "                        output[0, start_speech:i] = 0\n",
        "\n",
        "                start_speech = i\n",
        "\n",
        "            if output[1, i] == 0:\n",
        "                if i - start_music > 1:\n",
        "                    if (i - start_music) * duration_frame <= min_music:\n",
        "                        output[1, start_music:i] = 0\n",
        "\n",
        "                start_music = i\n",
        "        else:\n",
        "            if i - start_speech > 1:\n",
        "                if (i - start_speech) * duration_frame <= min_speech:\n",
        "                    output[0, start_speech:i + 1] = 0\n",
        "\n",
        "            if i - start_music > 1:\n",
        "                if (i - start_music) * duration_frame <= min_music:\n",
        "                    output[1, start_music:i + 1] = 0\n",
        "\n",
        "    return output\n",
        "\n",
        "\"\"\"\n",
        "This function converts the predictions made by the neural network into a format that is understood by sed_eval.\n",
        "\"\"\"\n",
        "\n",
        "def preds_to_se(p, audio_clip_length = 8.0):\n",
        "  start_speech = -100\n",
        "  start_music = -100\n",
        "  stop_speech = -100\n",
        "  stop_music = -100\n",
        "\n",
        "  audio_events = []\n",
        "\n",
        "  n_frames = p.shape[0]\n",
        "\n",
        "  if p[0, 0] == 1:\n",
        "    start_speech = 0\n",
        "  \n",
        "  if p[0, 1] == 1:\n",
        "    start_music = 0\n",
        "\n",
        "  for i in range(n_frames - 1):\n",
        "    if p[i, 0] == 0 and p[i + 1, 0] == 1:\n",
        "      start_speech = i + 1\n",
        "\n",
        "    elif p[i, 0] == 1 and p[i + 1, 0] == 0:\n",
        "      stop_speech = i\n",
        "      start_time = frames_to_time(start_speech)\n",
        "      stop_time = frames_to_time(stop_speech)\n",
        "      audio_events.append((start_time, stop_time, \"speech\"))\n",
        "      start_speech = -100\n",
        "      stop_speech = -100\n",
        "\n",
        "    if p[i, 1] == 0 and p[i + 1, 1] == 1:\n",
        "      start_music = i + 1\n",
        "    elif p[i, 1] == 1 and p[i + 1, 1] == 0:\n",
        "      stop_music = i\n",
        "      start_time = frames_to_time(start_music)\n",
        "      stop_time = frames_to_time(stop_music)      \n",
        "      audio_events.append((start_time, stop_time, \"music\"))\n",
        "      start_music = -100\n",
        "      stop_music = -100\n",
        "\n",
        "  if start_speech != -100:\n",
        "    start_time = frames_to_time(start_speech)\n",
        "    stop_time = audio_clip_length\n",
        "    audio_events.append((start_time, stop_time, \"speech\"))\n",
        "    start_speech = -100\n",
        "    stop_speech = -100\n",
        "\n",
        "  if start_music != -100:\n",
        "    start_time = frames_to_time(start_music)\n",
        "    stop_time = audio_clip_length\n",
        "    audio_events.append((start_time, stop_time, \"music\"))\n",
        "    start_music = -100\n",
        "    stop_music = -100\n",
        "\n",
        "  audio_events.sort(key = lambda x: x[0]) \n",
        "  return audio_events\n",
        "\n",
        "def frames_to_time(f, sr = 22050.0, hop_size = 220):\n",
        "  return f * hop_size / sr\n",
        "\n",
        "def get_log_melspectrogram(audio, sr = 22050, hop_length = 220, n_fft = 1024, n_mels = 128, fmin = 64, fmax = 8000):\n",
        "    \"\"\"Return the log-scaled Mel bands of an audio signal.\"\"\"\n",
        "    bands = librosa.feature.melspectrogram(\n",
        "        y=audio, sr=sr, hop_length=hop_length, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax, dtype=np.float32)\n",
        "    return librosa.core.power_to_db(bands, amin=1e-7)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Make predictions for full audio.\n",
        "\"\"\"\n",
        "def mk_preds_fa(audio_path, hop_size = 6.0, discard = 1.0, win_length = 8.0, sampling_rate = 22050):\n",
        "  in_signal, in_sr = sf.read(audio_path)\n",
        "\n",
        "  # Convert to mono if needed.\n",
        "  if (in_signal.ndim > 1):\n",
        "    in_sigal_mono = librosa.to_mono(in_signal)\n",
        "    in_signal = np.copy(in_signal_mono)\n",
        "  # Resample the audio file.\n",
        "  in_signal_22k = librosa.resample(in_signal, orig_sr=in_sr, target_sr=sampling_rate)\n",
        "  in_signal = np.copy(in_signal_22k)\n",
        "\n",
        "  audio_clip_length_samples = in_signal.shape[0]\n",
        "  print('audio_clip_length_samples is {}'.format(audio_clip_length_samples))\n",
        "\n",
        "  #hop_size_samples = int(hop_size * sampling_rate)\n",
        "  hop_size_samples = 220 * 602 - 1\n",
        "\n",
        "  #win_length_samples = int(win_length * sampling_rate)\n",
        "  win_length_samples = 220 * 802 - 1\n",
        "\n",
        "  n_preds = int(math.ceil((audio_clip_length_samples - win_length_samples) / hop_size_samples)) + 1\n",
        "\n",
        "  #print('n_preds is {}'.format(n_preds))\n",
        "\n",
        "  in_signal_pad = np.zeros((n_preds * hop_size_samples + 200 * 220))\n",
        "\n",
        "  #print('in_signal_pad.shape is {}'.format(in_signal_pad.shape))\n",
        "\n",
        "  in_signal_pad[0:audio_clip_length_samples] = in_signal\n",
        "\n",
        "  preds = np.zeros((n_preds, 802, 2))\n",
        "  mss_in = np.zeros((n_preds, 802, 128))\n",
        "\n",
        "  for i in range(n_preds):\n",
        "    seg = in_signal_pad[i * hop_size_samples:(i * hop_size_samples) + win_length_samples]\n",
        "    #print('seg.shape is {}'.format(seg.shape))\n",
        "    seg = librosa.util.normalize(seg)\n",
        "\n",
        "    mss = get_log_melspectrogram(seg)\n",
        "    M = mss.T\n",
        "    mss_in[i, :, :] = M\n",
        "\n",
        "  preds = (model.predict(mss_in) >= (0.5, 0.5)).astype(np.float)\n",
        "\n",
        "  preds_mid = np.copy(preds[1:-1, 100:702, :])\n",
        "\n",
        "  preds_mid_2 = preds_mid.reshape(-1, 2)\n",
        "\n",
        "  oa_preds = preds[0, 0:702, :] # oa stands for overall predictions\n",
        "\n",
        "  oa_preds = np.concatenate((oa_preds, preds_mid_2), axis = 0)\n",
        "\n",
        "  oa_preds = np.concatenate((oa_preds, preds[-1, 100:, :]), axis = 0)\n",
        "\n",
        "  return oa_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdzW04oyk_AF"
      },
      "source": [
        "parser = argparse.ArgumentParser(description=\"Music and speech detection on a given audio and output as txt file\")\n",
        "parser.add_argument('input_path', help='Input wav file path')\n",
        "parser.add_argument('output_path', help=\"Output txt file path\")\n",
        "\n",
        "args = parser.parse_args()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA4ITBuORNSd"
      },
      "source": [
        "test_audio = args.input_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mgghPfWoiCX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "20a96105-e401-4bcd-dfc3-8ebd8d9a0024"
      },
      "source": [
        "m = 'model 772_3.h5'\n",
        "\n",
        "eval_path = \"/content/drive/My Drive/ICASSP 2021/OpenBMAT/eval-2\"\n",
        "  \n",
        "model = tf.keras.models.load_model(m)\n",
        "\n",
        "ss, _ = sf.read(test_audio)\n",
        "oop = mk_preds_fa(test_audio)\n",
        "\n",
        "p_smooth = smooth_output(oop.T, min_speech=1.3, min_music=3.4, max_silence_speech=0.4, max_silence_music=0.6)\n",
        "p_smooth = p_smooth.T\n",
        "see = preds_to_se(p_smooth, audio_clip_length=ss.shape[0]/22050.0)\n",
        "\n",
        "n_label = args.output_path\n",
        "\n",
        "with open(n_label, 'w') as fp:\n",
        "  fp.write('\\n'.join('{},{},{}'.format(round(x[0], 5), round(x[1], 5), x[2]) for x in see))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/drive/My Drive/ICASSP 2021/e1/Models/model 128_1.h5']\n",
            "audio_clip_length_samples is 238140000\n",
            "Accuracy: 0.9377648704779312\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
# audio-seg-data-synth
Artificially synthesising data for audio segmentation to improve music-speech detection

This repository contains code associated with the paper titled "Artificially Synthesising Data for Audio Classification and Segmentation to improve speech and music detection in Radio Broadcast" submitted to IEEE ICASSP 2021.

# Disclaimer
Please note that the pre-trained models are for non-commercial use only. It has been trained on some datasets that do not permit commercial use. The entire list of datasets used for training are [MUSAN](http://www.openslr.org/17/), [GTZAN music-speech](http://marsyas.info/downloads/datasets.html), [GTZAN Genre collection](http://marsyas.info/downloads/datasets.html), [Scheirer & Slaney](https://labrosa.ee.columbia.edu/sounds/musp/scheislan.html), [Instrument Recognition in Musical Audio Signals](https://www.upf.edu/web/mtg/irmas#:~:text=IRMAS%20is%20intended%20to%20be,violin%2C%20and%20human%20singing%20voice.), [Singing Voice dataset](http://isophonics.net/SingingVoiceDataset), and  [LibriSpeech](http://www.openslr.org/12/).

# Quick Start
Want to try it without installing anything? Here is a [Google Colab notebook](https://colab.research.google.com/github/satvik-venkatesh/audio-seg-data-synth/blob/main/detection-example.ipynb).

# License
The pre-trained models are under the [Create Commons Attribution-NonCommercial-ShareAlike-3.0 Unported (CC BY-NC-SA 3.0) license](https://creativecommons.org/licenses/by-nc-sa/3.0/), and the source code for the project is under the [MIT license](https://github.com/satvik-venkatesh/audio-seg-data-synth/blob/main/LICENSE). 

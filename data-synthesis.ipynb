{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data-synthesis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMBwQ8Y29Z6FX6xmgkZuqxP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satvik-venkatesh/audio-seg-data-synth/blob/main/data-synthesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfofMt0CEqnM"
      },
      "source": [
        "# This is the final data synthesis file used for the paper titled \"Artificially Synthesising Data for Audio Classification and Segmentation to improve speech and music detection in Radio Broadcast\" submitted to ICASSP 2021."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjqpY5gaG3Xf"
      },
      "source": [
        "!pip install soundfile\n",
        "!sudo apt-get install sox\n",
        "!pip install pyloudnorm\n",
        "!pip install librosa==0.7.2\n",
        "from subprocess import Popen, PIPE\n",
        "import glob\n",
        "import numpy as np\n",
        "import random\n",
        "import soundfile as sf\n",
        "import pprint\n",
        "import json\n",
        "import pickle\n",
        "from shutil import copyfile\n",
        "import os\n",
        "import librosa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQAqc08gIybx"
      },
      "source": [
        "\"\"\"\n",
        "Mount Google Drive into Colab.\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkOLjqUa3Xlr"
      },
      "source": [
        "# Dowload all the datasets containing individual files of Music and Speech\n",
        "We do not have permission to share the datasets. However, all of them are either openly available or can be obtained from the authors of the datasets. After obtaining the dataset, it is a good idea a to store the .zip file in your personal google drive and use a !wget command to download it into this notebook. The transfer speed of Google drive is generally better than the original file server. Also, extracting the zip file directly from Google drive is slow for large files (> 5 GB)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQyYP3FOWHDr"
      },
      "source": [
        "\"\"\"\n",
        "Download the Musan database.\n",
        "\"\"\"\n",
        "file_size = 0\n",
        "while file_size < 10:\n",
        "  !wget http://www.openslr.org/resources/17/musan.tar.gz\n",
        "  file_size = os.path.getsize(\"musan.tar.gz\") / (1024 ** 2)\n",
        "  print(\"file_size is {}\".format(file_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SENd1QBh1OJO"
      },
      "source": [
        "!tar -xvf  \"/content/musan.tar.gz\" -C \"/content\"\n",
        "print(\"Completely extracted the files!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYsmW144R_8i"
      },
      "source": [
        "\"\"\"\n",
        "Remove all silences and resample the sound files.\n",
        "\"\"\"\n",
        "import glob\n",
        "music_list = glob.glob('/content/musan/music/**/*.wav', recursive = True)\n",
        "print(\"The contents of the music_list are: {}\".format(music_list))\n",
        "\n",
        "for sound in music_list:\n",
        "  temp_file = sound.replace('.wav', '_t.wav').replace('.WAV', '_t.WAV')\n",
        "  command = \"sox \" + sound + \" \" + temp_file + \" rate 22050 silence -l 1 0.1 1% -1 0.1 1%\"\n",
        "  p = Popen(command, stdin=PIPE, stdout=PIPE, stderr=PIPE, shell=True)\n",
        "  output, err = p.communicate()\n",
        "  copyfile(temp_file, sound)\n",
        "  os.remove(temp_file)\n",
        "\n",
        "\n",
        "speech_list = glob.glob('/content/musan/speech/**/*.wav', recursive = True)\n",
        "print(\"The contents of the speech_list are: {}\".format(speech_list))\n",
        "\n",
        "for sound in speech_list:\n",
        "  temp_file = sound.replace('.wav', '_t.wav').replace('.WAV', '_t.WAV')\n",
        "  command = \"sox \" + sound + \" \" + temp_file + \" rate 22050 silence -l 1 0.1 1% -1 0.1 1%\"\n",
        "  p = Popen(command, stdin=PIPE, stdout=PIPE, stderr=PIPE, shell=True)\n",
        "  output, err = p.communicate()\n",
        "  copyfile(temp_file, sound)\n",
        "  os.remove(temp_file)\n",
        "\n",
        "\n",
        "noise_list = glob.glob('/content/musan/noise/**/*.wav', recursive = True)\n",
        "print(\"The contents of the noise_list are: {}\".format(noise_list))\n",
        "\n",
        "\n",
        "for sound in noise_list:\n",
        "  temp_file = sound.replace('.wav', '_t.wav').replace('.WAV', '_t.WAV')\n",
        "  command = \"sox \" + sound + \" \" + temp_file + \" rate 22050\" # silence -l 1 0.1 1% -1 0.1 1%\"\n",
        "  p = Popen(command, stdin=PIPE, stdout=PIPE, stderr=PIPE, shell=True)\n",
        "  output, err = p.communicate()\n",
        "  copyfile(temp_file, sound)\n",
        "  os.remove(temp_file)\n",
        "\n",
        "\n",
        "# If there are files shorter than 9.1 s, loop them 4 times to increase their lengths.\n",
        "\n",
        "for sound in noise_list:\n",
        "  d, sr = sf.read(sound)\n",
        "  t = float(d.shape[0]) / sr\n",
        "  if t < 9.1:\n",
        "    temp_file = sound.replace('.wav', '_t.wav').replace('.WAV', '_t.WAV')\n",
        "    command = \"sox \" + sound + \" \" + temp_file + \" repeat 4\"\n",
        "    p = Popen(command, stdin=PIPE, stdout=PIPE, stderr=PIPE, shell=True)\n",
        "    output, err = p.communicate()\n",
        "    copyfile(temp_file, sound)\n",
        "    os.remove(temp_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO0aOLs1yE63"
      },
      "source": [
        "MUSAN is good for initial testing. To expand the data repository, you could download the [GTZAN music-speech](http://marsyas.info/downloads/datasets.html), [GTZAN Genre collection](http://marsyas.info/downloads/datasets.html), [Scheirer & Slaney](https://labrosa.ee.columbia.edu/sounds/musp/scheislan.html), [Instrument Recognition in Musical Audio Signals](https://www.upf.edu/web/mtg/irmas#:~:text=IRMAS%20is%20intended%20to%20be,violin%2C%20and%20human%20singing%20voice.), [Singing Voice dataset](http://isophonics.net/SingingVoiceDataset), and  [LibriSpeech (train-clean-100, dev-other)](http://www.openslr.org/12/). Please format the examples such that music and speech are stored in different folders. If the duration of the file is less than 9.1 s, loop the audio to get the required duration. This can be easily done using following command in SoX: \"sox \" + sound + \" \" + temp_file + \" repeat 3\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Orkt0BQjjwPg"
      },
      "source": [
        "print(len(music_list))\n",
        "print(len(speech_list))\n",
        "print(len(noise_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6Z5uSWlWsBS"
      },
      "source": [
        "music_files = glob.glob('/content/musan/music/**/*.wav', recursive = True)\n",
        "music_files.sort()\n",
        "speech_files = glob.glob('/content/musan/speech/**/*.wav', recursive = True)\n",
        "speech_files.sort()\n",
        "\n",
        "noise_files = glob.glob('/content/musan/noise/**/*.wav', recursive = True)\n",
        "noise_files.sort()\n",
        "\n",
        "music_files_filt = []\n",
        "speech_files_filt = []\n",
        "noise_files_filt = []\n",
        "\n",
        "min_dur = int(9.1 * 22050)\n",
        "\n",
        "for m in music_files:\n",
        "  a, sr = sf.read(m)\n",
        "  if a.shape[0] >= min_dur:\n",
        "    music_files_filt.append(m)\n",
        "\n",
        "for s in speech_files:\n",
        "  a, sr = sf.read(s)\n",
        "  if a.shape[0] >= min_dur:\n",
        "    speech_files_filt.append(s)\n",
        "\n",
        "for s in noise_files:\n",
        "  a, sr = sf.read(s)\n",
        "  if a.shape[0] >= min_dur:\n",
        "    noise_files_filt.append(s)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNYF9pNVxld5"
      },
      "source": [
        "music_files_filt.sort()\n",
        "speech_files_filt.sort()\n",
        "noise_files_filt.sort()\n",
        "\n",
        "print(len(music_files_filt))\n",
        "print(len(speech_files_filt))\n",
        "print(len(noise_files_filt))\n",
        "\n",
        "random.seed(4)\n",
        "random.shuffle(music_files_filt)\n",
        "random.shuffle(speech_files_filt)\n",
        "random.shuffle(noise_files_filt)\n",
        "\n",
        "m_music = len(music_files_filt)\n",
        "m_speech = len(speech_files_filt)\n",
        "m_noise = len(noise_files_filt)\n",
        "\n",
        "split_music = int(0.8 * m_music)\n",
        "split_speech = int(0.8 * m_speech)\n",
        "split_noise = int(0.8 * m_noise)\n",
        "\n",
        "# To synthesise training set\n",
        "music_list = music_files_filt[0:split_music]\n",
        "speech_list = speech_files_filt[0:split_speech]\n",
        "noise_list = noise_files_filt[0:split_noise]\n",
        "\n",
        "\"\"\"\n",
        "To synthesise validation set, uncomment the below lines\n",
        "(The original paper used manually annotated radio recordings as the validation set).\n",
        "\"\"\"\n",
        "# music_list = music_files_filt[split_music:]\n",
        "# speech_list = speech_files_filt[split_speech:]\n",
        "# noise_list = noise_files_filt[split_noise:] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52U60MZ3dXiY"
      },
      "source": [
        "print(len(music_list))\n",
        "print(len(speech_list))\n",
        "print(len(noise_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFeEZyWUB60C"
      },
      "source": [
        "random.seed()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4BQ5JIj4GAS"
      },
      "source": [
        "# Below is the process to synthesise data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n0UO0DY4dno"
      },
      "source": [
        "There are two types of examples --- (1) examples without background music and (2) examples with background music"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDcJ9xsY42xt"
      },
      "source": [
        "## Synthesis with background music"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oukH2RSxfYXS"
      },
      "source": [
        "\"\"\"\n",
        "This is the mixed (includes music + speech) version of create_transition\n",
        "\"\"\"\n",
        "def create_mixed_transition(max_f_out_dur = 1.0, max_f_in_dur = 1.0, max_c_fade_dur = 1.0, audio_clip_length = 8.0, min_segment_length = 1.0):\n",
        "    transition = {}\n",
        "    transition['type'] = random.choice([\"music+speech\", \"speech_to_music+speech\", \"music_to_music+speech\", \"music+speech_to_music\", \"music+speech_to_speech\"])\n",
        "    if transition['type'] == \"speech_to_music+speech\":\n",
        "      transition['music_gain'] = np.random.uniform(0.3, 0.7)\n",
        "      # transition['music_gain'] is a dummy value. It is set again later according to the loudness normalization.\n",
        "\n",
        "      transition['f_in_curve'] = random.choice(['linear', 'exp-convex', 'exp-concave', 's-curve'])\n",
        "      transition['f_in_dur'] = np.random.uniform(0, max_f_in_dur)\n",
        "      \n",
        "      if transition['f_in_curve'] == \"exp-convex\" or transition['f_in_curve'] == \"exp-concave\" or transition['f_in_curve'] == \"s-curve\":\n",
        "        transition['exp_value'] = np.random.uniform(1.5, 3.0) # This is the additional `exp_value` that is calculated only for exp and exp-convex transitions.\n",
        "        \n",
        "        \n",
        "    elif transition['type'] == \"music_to_music+speech\":\n",
        "      transition['music_gain_1'] = 1.0\n",
        "      #transition['music_gain_1'] = np.random.uniform(0.7, 1.0)\n",
        "      transition['music_gain_2'] = np.random.uniform(0.3, 0.7)\n",
        "      # transition['music_gain_2'] is a dummy value. It is set again later according to the loudness normalization.\n",
        "\n",
        "      transition['f_in_curve'] = random.choice(['linear', 'exp-convex', 'exp-concave', 's-curve'])\n",
        "      transition['f_in_dur'] = np.random.uniform(0, max_f_in_dur)\n",
        "      \n",
        "      if transition['f_in_curve'] == \"exp-convex\" or transition['f_in_curve'] == \"exp-concave\" or transition['f_in_curve'] == \"s-curve\":\n",
        "        transition['exp_value'] = np.random.uniform(1.5, 3.0) # This is the additional `exp_value` that is calculated only for exp and exp-convex transitions.\n",
        "\n",
        "      transition['f_out_curve'] = random.choice(['linear', 'exp-convex', 'exp-concave', 's-curve'])\n",
        "      transition['f_out_dur'] = np.random.uniform(0, max_f_out_dur)\n",
        "     \n",
        "      if transition['f_out_curve'] == \"exp-convex\" or transition['f_out_curve'] == \"exp-concave\" or transition['f_out_curve'] == \"s-curve\":\n",
        "          transition['exp_value'] = np.random.uniform(1.5, 3.0) # This is the additional `exp_value` that is calculated only for exp and exp-convex transitions.\n",
        "\n",
        "\n",
        "    elif transition['type'] == \"music+speech_to_music\":\n",
        "      transition['music_gain_1'] = np.random.uniform(0.3, 0.7)\n",
        "      # transition['music_gain_1'] is a dummy value. It is set again later according to the loudness normalization.\n",
        "      transition['music_gain_2'] = 1.0\n",
        "\n",
        "      transition['f_out_curve'] = random.choice(['linear', 'exp-convex', 'exp-concave', 's-curve'])\n",
        "      transition['f_out_dur'] = np.random.uniform(0, max_f_out_dur)\n",
        "     \n",
        "      if transition['f_out_curve'] == \"exp-convex\" or transition['f_out_curve'] == \"exp-concave\" or transition['f_out_curve'] == \"s-curve\":\n",
        "          transition['exp_value'] = np.random.uniform(1.5, 3.0) # This is the additional `exp_value` that is calculated only for exp and exp-convex transitions.\n",
        "\n",
        "      transition['f_in_curve'] = random.choice(['linear', 'exp-convex', 'exp-concave', 's-curve'])\n",
        "      transition['f_in_dur'] = np.random.uniform(0, max_f_in_dur)\n",
        "      \n",
        "      if transition['f_in_curve'] == \"exp-convex\" or transition['f_in_curve'] == \"exp-concave\" or transition['f_in_curve'] == \"s-curve\":\n",
        "        transition['exp_value'] = np.random.uniform(1.5, 3.0) # This is the additional `exp_value` that is calculated only for exp and exp-convex transitions.\n",
        "\n",
        "    elif transition['type'] == \"music+speech_to_speech\":\n",
        "      transition['music_gain'] = np.random.uniform(0.3, 0.7)\n",
        "\n",
        "      transition['f_out_curve'] = random.choice(['linear', 'exp-convex', 'exp-concave', 's-curve'])\n",
        "      transition['f_out_dur'] = np.random.uniform(0, max_f_out_dur)\n",
        "\n",
        "      if transition['f_out_curve'] == \"exp-convex\" or transition['f_out_curve'] == \"exp-concave\" or transition['f_out_curve'] == \"s-curve\":\n",
        "          transition['exp_value'] = np.random.uniform(1.5, 3.0) # This is the additional `exp_value` that is calculated only for exp and exp-convex transitions.\n",
        "\n",
        "    elif transition['type'] == \"music+speech\":\n",
        "      transition['music_gain'] = np.random.uniform(0.3, 0.7)\n",
        "      # transition['music_gain'] is a dummy value. It is set again later according to the loudness normalization.\n",
        "\n",
        "    # ======= Calculate the time of transition in the same function ========\n",
        "    if transition['type'] == \"music+speech\":\n",
        "      return (transition, -1.0)\n",
        "    \n",
        "    else:\n",
        "      point = np.random.uniform(min_segment_length + max_f_out_dur, audio_clip_length - min_segment_length - max_f_in_dur)\n",
        "      return (transition, point)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbTqPCI2_7or"
      },
      "source": [
        "def create_mixed_samples_list(music_sounds, speech_sounds):\n",
        "    \"\"\"\n",
        "    Returns a dictionary containing music and speech sounds.\n",
        "    Take the class_list as input and randomly pick sound files in the `music_sounds` and `speech_sounds` folder.\n",
        "    \"\"\"\n",
        "    samples = {}\n",
        "\n",
        "    cc = random.choice(music_sounds)\n",
        "    samples['music'] = cc\n",
        "\n",
        "    cc = random.choice(speech_sounds)\n",
        "    samples['speech'] = cc\n",
        "\n",
        "    return samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slS6PELZDN7R"
      },
      "source": [
        "def get_mixed_segment_lengths(transition, audio_clip_length=8.0, sr = 22050):\n",
        "  \"\"\"\n",
        "  This function returns a dictionary.\n",
        "  \"\"\"\n",
        "  segment_lengths = {}\n",
        "\n",
        "  ac_len_samples = int(audio_clip_length * sr)\n",
        "  t_samples = int(transition[1] * sr) # Transition time in samples\n",
        "\n",
        "  if transition[0]['type'] == \"music+speech\":\n",
        "    segment_lengths['music'] = ac_len_samples\n",
        "    segment_lengths['speech'] = ac_len_samples\n",
        "\n",
        "  elif transition[0]['type'] == \"speech_to_music+speech\":\n",
        "    segment_lengths['speech'] = ac_len_samples\n",
        "    segment_lengths['music'] = ac_len_samples - t_samples\n",
        "\n",
        "  elif transition[0]['type'] == \"music_to_music+speech\":\n",
        "    segment_lengths['speech'] = ac_len_samples - t_samples\n",
        "    segment_lengths['music'] = ac_len_samples\n",
        "\n",
        "  elif transition[0]['type'] == \"music+speech_to_music\":\n",
        "    segment_lengths['music'] = ac_len_samples\n",
        "    segment_lengths['speech'] = t_samples\n",
        "\n",
        "  elif transition[0]['type'] == \"music+speech_to_speech\":\n",
        "    segment_lengths['speech'] = ac_len_samples\n",
        "    segment_lengths['music'] = t_samples\n",
        "\n",
        "  return segment_lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eqe_0ZSZJ79J"
      },
      "source": [
        "def get_mixed_random_segments(samples, segment_lengths, f_buffer = 0.0, sr = 22050):\n",
        "    \"\"\"\n",
        "    This function returns a dictionary of tuples that specifies the segment boundaries in the original sound file.\n",
        "    \"\"\"   \n",
        "\n",
        "    f_buffer_samples = int(f_buffer * sr)\n",
        "\n",
        "    segments = {}     \n",
        "    d, sr = sf.read(samples['speech'])\n",
        "    sample_length = len(d)\n",
        "    r = np.random.randint(f_buffer_samples, sample_length - segment_lengths['speech'] - f_buffer_samples)\n",
        "    segments['speech'] = (r, r + segment_lengths['speech'])\n",
        "    \n",
        "    d, sr = sf.read(samples['music'])\n",
        "    sample_length = len(d)\n",
        "    r = np.random.randint(f_buffer_samples, sample_length - segment_lengths['music'] - f_buffer_samples)\n",
        "    segments['music'] = (r, r + segment_lengths['music'])\n",
        "\n",
        "    return segments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHhln0NKkPN-"
      },
      "source": [
        "def apply_mixed_fade_out(audio, transition, sr=22050.0, end_gain = 0.0):\n",
        "  stop = audio.shape[0]\n",
        "  f_out_length_samples =  int(transition[0]['f_out_dur'] * sr)\n",
        "\n",
        "  if transition[0]['f_out_curve'] == \"linear\":     \n",
        "      audio[stop - f_out_length_samples:stop] = audio[stop - f_out_length_samples:stop] * np.linspace(1.0, end_gain, num = f_out_length_samples)\n",
        "\n",
        "  elif transition[0]['f_out_curve'] == \"exp-concave\":\n",
        "      a = np.linspace(1.0, 0.0, num = f_out_length_samples)\n",
        "      x = transition[0]['exp_value']\n",
        "      fade_curve = a ** x\n",
        "      fade_curve = fade_curve * (1 - end_gain) + end_gain\n",
        "      audio[stop - f_out_length_samples:stop] = audio[stop - f_out_length_samples:stop] * fade_curve\n",
        "      \n",
        "  elif transition[0]['f_out_curve'] == \"exp-convex\":\n",
        "      a = np.linspace(0.0, 1.0, num = f_out_length_samples)\n",
        "      x = transition[0]['exp_value']\n",
        "      fade_curve = 1 - a ** x\n",
        "      fade_curve = fade_curve * (1 - end_gain) + end_gain\n",
        "      audio[stop - f_out_length_samples:stop] = audio[stop - f_out_length_samples:stop] * fade_curve\n",
        "      \n",
        "  elif transition[0]['f_out_curve'] == \"s-curve\":\n",
        "      n_1 = int(f_out_length_samples / 2)\n",
        "      a_1 = np.linspace(0.0, 1.0, num = n_1)\n",
        "      a_2 = np.linspace(0.0, 1.0, num = f_out_length_samples - n_1)\n",
        "      x = transition[0]['exp_value']\n",
        "      \n",
        "      convex = 0.5 * (1 - a_1 ** x) + 0.5\n",
        "      \n",
        "      concave = 0.5 * (1 - a_2)  ** x\n",
        "      \n",
        "      fade_curve = np.concatenate((convex, concave))\n",
        "      fade_curve = fade_curve * (1 - end_gain) + end_gain\n",
        "      \n",
        "      audio[stop - f_out_length_samples:stop] = audio[stop - f_out_length_samples:stop] * fade_curve"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opENnrNzm9cG"
      },
      "source": [
        "def apply_mixed_normal_fade_in(audio, transition, sr=22050.0, end_gain = 1.0, start_gain = 0.0):\n",
        "  start = 0\n",
        "  f_in_length_samples =  int(transition[0]['f_in_dur'] * sr)  \n",
        "\n",
        "  #print('f_in_length_samples is {}'.format(f_in_length_samples))\n",
        "\n",
        "  #print(\"audio.shape is {}\".format(audio.shape))\n",
        "\n",
        "  if transition[0]['f_in_curve'] == \"linear\":        \n",
        "    audio[start:start + f_in_length_samples] = audio[start:start + f_in_length_samples] * np.linspace(start_gain, end_gain, num = f_in_length_samples)      \n",
        "\n",
        "  elif transition[0]['f_in_curve'] == \"exp-concave\":\n",
        "    a = np.linspace(0.0, 1.0, num = f_in_length_samples)\n",
        "    x = transition[0]['exp_value']\n",
        "    fade_curve = a ** x\n",
        "    fade_curve = fade_curve * (end_gain - start_gain) + start_gain\n",
        "    audio[start:start + f_in_length_samples] = audio[start:start + f_in_length_samples] * fade_curve\n",
        "      \n",
        "  elif transition[0]['f_in_curve'] == \"exp-convex\":\n",
        "    a = np.linspace(1.0, 0.0, num = f_in_length_samples)\n",
        "    x = transition[0]['exp_value']\n",
        "    fade_curve = 1 - a ** x\n",
        "    fade_curve = fade_curve * (end_gain - start_gain) + start_gain\n",
        "    audio[start:start + f_in_length_samples] = audio[start:start + f_in_length_samples] * fade_curve\n",
        "      \n",
        "  elif transition[0]['f_in_curve'] == \"s-curve\":\n",
        "    n_1 = int(f_in_length_samples / 2)\n",
        "    a_1 = np.linspace(0.0, 1.0, num = n_1)\n",
        "    a_2 = np.linspace(0.0, 1.0, num = f_in_length_samples - n_1)\n",
        "    x = transition[0]['exp_value']\n",
        "    \n",
        "    concave = 0.5 * a_1 ** x\n",
        "    \n",
        "    convex = 0.5 * (1 - (1 - a_2)  ** x) + 0.5\n",
        "    \n",
        "    fade_curve = np.concatenate((concave, convex))\n",
        "    fade_curve = fade_curve * (end_gain - start_gain) + start_gain\n",
        "    audio[start:start + f_in_length_samples] = audio[start:start + f_in_length_samples] * fade_curve"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av-CaLBv-xoJ"
      },
      "source": [
        "def generate_mixed_multiclass_labels(transition, audio_clip_length = 8.0, sr = 22050.0, res = 220):\n",
        "  \"\"\"\n",
        "  This function generates multiclass labels for music+speech examples.\n",
        "  `res` is in samples.\n",
        "  \"\"\"\n",
        "  res_t = 220 / sr\n",
        "  no_of_labels = int(np.ceil(audio_clip_length / res_t))\n",
        "  t_point = int(transition[1] / res_t)\n",
        "  \n",
        "  if 'f_out_dur' in transition[0]:\n",
        "    f_out_samples = int(transition[0]['f_out_dur'] / res_t)\n",
        "  \n",
        "  if 'f_in_dur' in transition[0]:\n",
        "    f_in_samples = int(transition[0]['f_in_dur'] / res_t)\n",
        "\n",
        "  labels = np.zeros((no_of_labels, 2), dtype = np.int16)\n",
        "\n",
        "  \"\"\"\n",
        "  \"music+speech\", \"speech_to_music+speech\", \"music_to_music+speech\", \"music+speech_to_music\", \"music+speech_to_speech\"\n",
        "  \"\"\"\n",
        "\n",
        "  if transition[0]['type'] == \"music+speech\":\n",
        "    labels[:, 0] = 1\n",
        "    labels[:, 1] = 1\n",
        "\n",
        "  elif transition[0]['type'] == \"speech_to_music+speech\":\n",
        "    labels[:, 0] = 1\n",
        "    labels[t_point:, 1] = 1\n",
        "\n",
        "  elif transition[0]['type'] == \"music_to_music+speech\":\n",
        "    labels[t_point:, 0] = 1\n",
        "    labels[:, 1] = 1\n",
        "\n",
        "  elif transition[0]['type'] == \"music+speech_to_music\":\n",
        "    labels[0:t_point, 0] = 1\n",
        "    labels[:, 1] = 1\n",
        "\n",
        "  elif transition[0]['type'] == \"music+speech_to_speech\":\n",
        "    labels[:, 0] = 1\n",
        "    labels[0:t_point, 1] = 1\n",
        "  \n",
        "  return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LgSB_bP0vGY"
      },
      "source": [
        "import pyloudnorm as pyln\n",
        "def get_random_loudness_gain(speech_data, music_data, rate = 22050):\n",
        "  meter = pyln.Meter(rate)\n",
        "  speech_loudness = meter.integrated_loudness(speech_data)\n",
        "  music_loudness = meter.integrated_loudness(music_data)\n",
        "  random_loudness = np.random.uniform(speech_loudness - 18.0, speech_loudness - 7.0)\n",
        "  delta_loudness = random_loudness - music_loudness\n",
        "  gain = np.power(10.0, delta_loudness/20.0)\n",
        "\n",
        "  #print(\"The gain is {}\".format(gain))\n",
        "\n",
        "  return gain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2rqqJrSk5hR"
      },
      "source": [
        "\"\"\"\n",
        "This would create an audio clip template based on the output of create_mixed_transition.\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "\"music+speech\", \"speech_to_music+speech\", \"music_to_music+speech\", \"music+speech_to_music\", \"music+speech_to_speech\"\n",
        "\"\"\"\n",
        "\n",
        "def create_mixed_audio_clip(audio_clip_length = 8.0, sr = 22050.0):\n",
        "  transition = create_mixed_transition()\n",
        "  #print(transition)\n",
        "  samples = create_mixed_samples_list(music_sounds, speech_sounds)\n",
        "  segment_lengths = get_mixed_segment_lengths(transition)\n",
        "  segments = get_mixed_random_segments(samples, segment_lengths)\n",
        "\n",
        "  #print(segments)\n",
        "\n",
        "  start_sp = segments['speech'][0]\n",
        "  stop_sp = segments['speech'][1]\n",
        "\n",
        "  start_mu = segments['music'][0]\n",
        "  stop_mu = segments['music'][1]\n",
        "\n",
        "  if transition[0]['type'] == \"music+speech\":\n",
        "    synth_audio, _ = sf.read(samples['speech'], start = start_sp, stop = stop_sp)\n",
        "    synth_audio = librosa.util.normalize(synth_audio)\n",
        "\n",
        "    synth_music, _ = sf.read(samples['music'], start = start_mu, stop = stop_mu)\n",
        "    synth_music = librosa.util.normalize(synth_music)\n",
        "\n",
        "    m_gain = get_random_loudness_gain(synth_audio, synth_music)\n",
        "\n",
        "    synth_audio += m_gain * synth_music\n",
        "\n",
        "  elif transition[0]['type'] == \"speech_to_music+speech\":\n",
        "    synth_audio, _ = sf.read(samples['speech'], start = start_sp, stop = stop_sp)\n",
        "    synth_audio = librosa.util.normalize(synth_audio)\n",
        "\n",
        "    music_start_point = int(transition[1] * sr)\n",
        "\n",
        "    synth_music, _ = sf.read(samples['music'], start = start_mu, stop = stop_mu)\n",
        "    synth_music = librosa.util.normalize(synth_music)\n",
        "\n",
        "    m_gain = get_random_loudness_gain(synth_audio, synth_music)\n",
        "\n",
        "    apply_mixed_normal_fade_in(synth_music, transition, sr=22050.0, end_gain = m_gain)\n",
        "\n",
        "    f_in_length_samples =  int(transition[0]['f_in_dur'] * sr)  \n",
        "\n",
        "    synth_music[f_in_length_samples:] = synth_music[f_in_length_samples:] * m_gain\n",
        "\n",
        "    synth_audio[music_start_point:] += synth_music\n",
        "\n",
        "  elif transition[0]['type'] == \"music_to_music+speech\":\n",
        "    synth_audio, _ = sf.read(samples['music'], start = start_mu, stop = stop_mu)\n",
        "    synth_audio = librosa.util.normalize(synth_audio)\n",
        "    \n",
        "    music1_end_point = int(transition[1] * sr)\n",
        "    synth_music1 = synth_audio[0:music1_end_point]\n",
        "    synth_music1 = synth_music1 * transition[0]['music_gain_1']\n",
        "    #print('synth_music1.shape is {}'.format(synth_music1.shape))\n",
        "\n",
        "    synth_speech, _ = sf.read(samples['speech'], start = start_sp, stop = stop_sp)\n",
        "    synth_speech = librosa.util.normalize(synth_speech)\n",
        "\n",
        "    apply_mixed_normal_fade_in(synth_speech, transition, sr=22050.0)\n",
        "\n",
        "    m_gain = get_random_loudness_gain(synth_speech, synth_audio)\n",
        "    apply_mixed_fade_out(synth_music1, transition, sr=22050.0, end_gain = m_gain)\n",
        "\n",
        "    synth_music2 = synth_audio[music1_end_point:]\n",
        "    synth_music2 = synth_music2 * m_gain\n",
        "\n",
        "    synth_audio[0:music1_end_point] = synth_music1\n",
        "    synth_audio[music1_end_point:] = synth_speech + synth_music2\n",
        "\n",
        "  elif transition[0]['type'] == \"music+speech_to_music\":\n",
        "    synth_audio, _ = sf.read(samples['music'], start = start_mu, stop = stop_mu)\n",
        "    synth_audio = librosa.util.normalize(synth_audio)\n",
        "\n",
        "    speech_start_point = int(transition[1] * sr)\n",
        "\n",
        "    synth_speech, _ = sf.read(samples['speech'], start = start_sp, stop = stop_sp)\n",
        "    synth_speech = librosa.util.normalize(synth_speech)\n",
        "\n",
        "    apply_mixed_fade_out(synth_speech, transition, sr=22050.0)\n",
        "\n",
        "    music1_end_point = int(transition[1] * sr)\n",
        "    synth_music1 = synth_audio[0:music1_end_point]\n",
        "\n",
        "    m_gain = get_random_loudness_gain(synth_speech, synth_audio)\n",
        "\n",
        "    synth_music1 = synth_music1 * m_gain\n",
        "\n",
        "    synth_music2 = synth_audio[music1_end_point:]\n",
        "    f_in_length_samples =  int(transition[0]['f_in_dur'] * sr)\n",
        "    apply_mixed_normal_fade_in(synth_music2, transition, sr=22050.0, start_gain = m_gain, end_gain = transition[0]['music_gain_2'])\n",
        "    synth_music2[f_in_length_samples:] = synth_music2[f_in_length_samples:] * transition[0]['music_gain_2']\n",
        "\n",
        "    synth_audio[0:music1_end_point] = synth_speech + synth_music1\n",
        "\n",
        "    synth_audio[music1_end_point:] = synth_music2\n",
        "\n",
        "  elif transition[0]['type'] == \"music+speech_to_speech\":\n",
        "    synth_audio, _ = sf.read(samples['speech'], start = start_sp, stop = stop_sp)\n",
        "    synth_audio = librosa.util.normalize(synth_audio)\n",
        "\n",
        "    music_end_point = int(transition[1] * sr)\n",
        "\n",
        "    synth_music, _ = sf.read(samples['music'], start = start_mu, stop = stop_mu)\n",
        "    synth_music = librosa.util.normalize(synth_music)\n",
        "\n",
        "    m_gain = get_random_loudness_gain(synth_audio, synth_music)\n",
        "\n",
        "    synth_music = synth_music * m_gain\n",
        "    apply_mixed_fade_out(synth_music, transition, sr=22050.0)\n",
        "\n",
        "    synth_audio[0:music_end_point] += synth_music\n",
        "\n",
        "  return (synth_audio, transition)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVtQREP_hnR1"
      },
      "source": [
        "def get_log_melspectrogram(audio, sr = 22050, hop_length = 220, n_fft = 1024, n_mels = 80, fmin = 64, fmax = 8000):\n",
        "    \"\"\"Return the log-scaled Mel bands of an audio signal.\"\"\"\n",
        "    bands = librosa.feature.melspectrogram(\n",
        "        y=audio, sr=sr, hop_length=hop_length, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax, dtype=np.float32)\n",
        "    return librosa.core.power_to_db(bands, amin=1e-7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmPbJOcJHBYA"
      },
      "source": [
        "\"\"\"\n",
        "This function is used to divide the synthesised into different folders called blocks.\n",
        "\"\"\"\n",
        "\n",
        "def get_block_id(mel_id, block_size = 320):\n",
        "  i = int(mel_id.replace(\"mel-id-\", \"\"))\n",
        "  b = int((i - 1) // block_size)\n",
        "  return b + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnO-uj05yUi9"
      },
      "source": [
        "## Synthesis without background music"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJWhwTLzyZrL"
      },
      "source": [
        "def check_overlap(transition_points, point, min_segment_length):\n",
        "    is_overlap = False\n",
        "    for t in transition_points:\n",
        "        if np.absolute(point - t) <= min_segment_length + 2.0: # I am adding 4.0 to separate out the files.\n",
        "            is_overlap = True\n",
        "    return is_overlap\n",
        "\n",
        "def create_random_transition_points(audio_clip_length, min_segment_length = 1.0, max_f_out_dur = 0.5, max_f_in_dur = 0.0):   \n",
        "    # If max_no_transitions = 2, then the audio example can have maximum of 1 transition (ie., max_no_transitions - 1)\n",
        "    max_no_transitions = 2\n",
        "    number_of_transitions = np.random.randint(0, max_no_transitions)\n",
        "    #print(\"Number of transitions is {}\".format(number_of_transitions))\n",
        "    if number_of_transitions == 0:\n",
        "        return []\n",
        "    transition_points = [np.random.uniform(min_segment_length + max_f_out_dur, audio_clip_length - min_segment_length - max_f_in_dur)]\n",
        "    #print(transition_points)\n",
        "    \n",
        "    # Limit number of iterations \n",
        "    num_iters = 100000\n",
        "    \n",
        "    while len(transition_points) < number_of_transitions:\n",
        "        point = np.random.uniform(min_segment_length + max_f_out_dur, audio_clip_length - min_segment_length - max_f_in_dur)\n",
        "        if not check_overlap(transition_points, point, min_segment_length):\n",
        "            transition_points.append(point)\n",
        "            num_iters = 100000\n",
        "        else: \n",
        "            num_iters -= 1\n",
        "            if num_iters < 0:\n",
        "                #print('Unable to find the required number of transition points. The minimum segment length seems to be high!!')\n",
        "                \n",
        "                # Re-calculate the number of transitions and the first transition_point.\n",
        "                number_of_transitions = np.random.randint(0, max_no_transitions)\n",
        "                #print(\"Number of transitions is {}\".format(number_of_transitions))\n",
        "                if number_of_transitions == 0:\n",
        "                    return []\n",
        "                transition_points = [np.random.uniform(min_segment_length + max_f_out_dur, audio_clip_length - min_segment_length - max_f_in_dur)]\n",
        "                #print(transition_points)\n",
        "                continue                \n",
        "                raise ValueError('Unable to find the required number of transition points. The minimum segment length seems to be high!!')\n",
        "    transition_points.sort()\n",
        "    return transition_points\n",
        "\n",
        "def create_transition(max_f_out_dur = 1.0, max_f_in_dur = 1.0, max_c_fade_dur = 1.0, max_time_gap = 0.2):\n",
        "    \"\"\"\n",
        "    Returns a dictionary containing parameters of the transition.\n",
        "    For a normal fade, it is the following.\n",
        "    {type, f_out_curve, f_out_dur, time_gap, f_in_curve, f_in_dur}.\n",
        "    For a cross-fade it is the following.\n",
        "    {type, f_out_curve, f_out_dur, f_in_curve, f_in_dur}.\n",
        "    \"\"\"\n",
        "    transition = {}\n",
        "    transition['type'] = random.choice(['normal', 'cross-fade'])\n",
        "    if transition['type'] == \"normal\":\n",
        "        transition['f_out_curve'] = random.choice(['linear', 'exp-convex', 'exp-concave', 's-curve'])\n",
        "        transition['f_out_dur'] = np.random.uniform(0, max_f_out_dur)\n",
        "        transition['time_gap'] = np.random.uniform(0.0, max_time_gap) # I am setting this to only positive values for the moment.\n",
        "        transition['f_in_curve'] = random.choice(['linear', 'exp-convex', 'exp-concave', 's-curve'])\n",
        "        transition['f_in_dur'] = np.random.uniform(0, max_f_in_dur)\n",
        "        \n",
        "        if transition['f_out_curve'] == \"exp-convex\" or transition['f_out_curve'] == \"exp-concave\" or transition['f_out_curve'] == \"s-curve\":\n",
        "            transition['exp_value'] = np.random.uniform(1.5, 3.0) # This is the additional `exp_value` that is calculated only for exp and exp-convex transitions.\n",
        "\n",
        "        if transition['f_in_curve'] == \"exp-convex\" or transition['f_in_curve'] == \"exp-concave\" or transition['f_in_curve'] == \"s-curve\":\n",
        "            transition['exp_value'] = np.random.uniform(1.5, 3.0) # This is the additional `exp_value` that is calculated only for exp and exp-convex transitions.\n",
        "        \n",
        "        \n",
        "    elif transition['type'] == \"cross-fade\":\n",
        "        transition['f_out_curve'] = random.choice(['linear', 'exp-convex', 'exp-concave', 's-curve'])\n",
        "        transition['f_out_dur'] = np.random.uniform(0, max_c_fade_dur)\n",
        "        transition['f_in_curve'] = random.choice(['linear', 'exp-convex', 'exp-concave', 's-curve'])\n",
        "        transition['f_in_dur'] = np.random.uniform(0, max_c_fade_dur)        \n",
        "\n",
        "        if transition['f_out_curve'] == \"exp-convex\" or transition['f_out_curve'] == \"exp-concave\" or transition['f_out_curve'] == \"s-curve\":\n",
        "            transition['exp_value'] = np.random.uniform(1.5, 3.0) # This is the additional `exp_value` that is calculated only for exp and exp-convex transitions.\n",
        "\n",
        "        if transition['f_in_curve'] == \"exp-convex\" or transition['f_in_curve'] == \"exp-concave\" or transition['f_in_curve'] == \"s-curve\":\n",
        "            transition['exp_value'] = np.random.uniform(1.5, 3.0) # This is the additional `exp_value` that is calculated only for exp-concave and exp-convex transitions.\n",
        "    \n",
        "    #print(transition)\n",
        "    return transition\n",
        "\n",
        "def create_transition_list(transition_points):\n",
        "    \"\"\"\n",
        "    This function returns a list of transitions. \n",
        "    Each element in the list is tuple (transition, time_stamp)\n",
        "    \"\"\"\n",
        "    transitions_list = []\n",
        "    for i in range(len(transition_points)):\n",
        "      if len(transition_points) == 1 or i == len(transition_points) - 1:\n",
        "        t = create_transition(max_time_gap = transition_points[i] - 1.0)\n",
        "        transitions_list.append((t, transition_points[i]))\n",
        "\n",
        "      elif i == 0:\n",
        "        #print(\"Central section reached!!!\")\n",
        "        s_len = transition_points[i]\n",
        "        #print(\"s_len is {}\".format(s_len))\n",
        "        t = create_transition(max_f_out_dur=min(0.2 * s_len, 1.0), max_f_in_dur=min(0.1 * s_len, 1.0), max_c_fade_dur=min(0.1 * s_len, 1.0))\n",
        "        transitions_list.append((t, transition_points[i]))        \n",
        "      else:\n",
        "        #print(\"Middle section reached!!!\")\n",
        "        s_len = transition_points[i] - transition_points[i - 1]\n",
        "        #print(\"s_len is {}\".format(s_len))\n",
        "        t = create_transition(max_f_out_dur=min(0.2 * s_len, 1.0), max_f_in_dur=min(0.1 * s_len, 1.0), max_c_fade_dur=min(0.1 * s_len, 1.0), max_time_gap = transition_points[i] - 1.0)\n",
        "        transitions_list.append((t, transition_points[i]))\n",
        "\n",
        "    return transitions_list\n",
        "\n",
        "def create_class_list(no_of_classes):\n",
        "    \"\"\"\n",
        "    Create a random list of classes.\n",
        "    \"\"\"\n",
        "    # Create a random list containing music or speech.\n",
        "    class_list = random.choices(['music', 'speech', 'noise'], weights=[0.4, 0.4, 0.2], k=no_of_classes)\n",
        "    return class_list\n",
        "\n",
        "def create_samples_list(class_list, music_sounds, speech_sounds, noise_sounds):\n",
        "    \"\"\"\n",
        "    Take the class_list as input and randomly pick sound files in the `music_sounds` and `speech_sounds` folder.\n",
        "    \"\"\"\n",
        "    samples_list = []\n",
        "    for c in class_list:\n",
        "        if (c == \"music\"):\n",
        "            cc = random.choice(music_sounds)\n",
        "            samples_list.append(cc)\n",
        "        elif (c == \"speech\"):\n",
        "            cc = random.choice(speech_sounds)\n",
        "            samples_list.append(cc)\n",
        "        elif (c == \"noise\"):\n",
        "            cc = random.choice(noise_sounds)\n",
        "            samples_list.append(cc)\n",
        "\n",
        "        else:\n",
        "            print(\"Encountered unexpected class!!\")\n",
        "            raise ValueError(\"Encountered unexpected class!!\")\n",
        "    return samples_list\n",
        "\n",
        "def get_segment_lengths(transitions_list, audio_clip_length):\n",
        "    \"\"\"\n",
        "    This function takes the list of transitions as input.\n",
        "    It returns the length of each segment.\n",
        "    \"\"\"\n",
        "    segment_lengths = []\n",
        "    # Extract the time_stamps from transitions_list\n",
        "    time_stamps = [j for (i, j) in transitions_list]\n",
        "    time_stamps = [0] + time_stamps + [audio_clip_length]\n",
        "    for t in range(len(time_stamps) - 1):\n",
        "        tt = time_stamps[t + 1] - time_stamps[t]\n",
        "        segment_lengths.append(tt)    \n",
        "    return segment_lengths\n",
        "\n",
        "def get_random_segments(samples_list, segment_lengths, f_buffer = 1.1):\n",
        "    \"\"\"\n",
        "    This function picks random segments from the samples_list. \n",
        "    It returns a list of tuples (segment_start, segment_end)\n",
        "    \"\"\"\n",
        "    if len(samples_list) != len(segment_lengths):\n",
        "        print(\"The length of samples_list needs to be equal to segment_lengths!!\")\n",
        "        raise ValueError(\"Data mismatch --- The length of samples_list needs to be equal to segment_lengths!!\")\n",
        "    \n",
        "    segments = []\n",
        "    \n",
        "    for i in range(len(samples_list)):\n",
        "        #remove_silence_and_resample(samples_list[i])\n",
        "        \n",
        "        d, sr = sf.read(samples_list[i])\n",
        "        sample_length = float(len(d) / sr)\n",
        "        r = np.random.uniform(f_buffer, sample_length - segment_lengths[i] - f_buffer)\n",
        "        segments.append((r, r + segment_lengths[i]))\n",
        "    return segments\n",
        "\n",
        "def create_template_audio_clip(audio_clip_length, samples_list, segments, sr):\n",
        "    \"\"\"\n",
        "    This stitches all the individual audio segments into one file. It does not include the transitions.\n",
        "    It returns `synth_audio` which is the synthesised audio file.\n",
        "    It also returns `synth_audio_seg_samples`, which is a list of tuples (audio clip start, audio clip stop).\n",
        "    These tuples serve as reference points to perform fade in and fade out operations.\n",
        "    \"\"\"\n",
        "    if (len(samples_list) < 1):\n",
        "        print(\"The samples_list argument is invalid!!\")\n",
        "        raise ValueError(\"The samples_list argument is invalid!!\")\n",
        "    \n",
        "    synth_audio_seg_samples = [] # This is a list of tuples containing segment boundaries in the synthesised audio.\n",
        "    \n",
        "    #print(\"segments[0]: {}\".format(segments[0]))\n",
        "    \n",
        "    start = int(segments[0][0] * sr)\n",
        "    stop = int(np.ceil(segments[0][1] * sr))\n",
        "    ac_start = 0 # Synthesised audio clip start\n",
        "    ac_stop = stop - start\n",
        "    synth_audio_seg_samples.append((ac_start, ac_stop))\n",
        "    synth_audio, _ = sf.read(samples_list[0], start = start, stop = stop)\n",
        "    synth_audio = librosa.util.normalize(synth_audio)\n",
        "\n",
        "    for i in range(1, len(samples_list)):\n",
        "\n",
        "        start = int(segments[i][0] * sr)\n",
        "        stop = int(np.ceil(segments[i][1] * sr))\n",
        "        ac_start = ac_stop # I have just removed the `+ 1` from the equation.\n",
        "        ac_stop = ac_start + stop - start\n",
        "        synth_audio_seg_samples.append((ac_start, ac_stop))\n",
        "        sa, _ = sf.read(samples_list[i], start = start, stop = stop)\n",
        "        sa = librosa.util.normalize(sa)\n",
        "        synth_audio = np.concatenate((synth_audio, sa), axis = 0)\n",
        "    \n",
        "    return synth_audio, synth_audio_seg_samples\n",
        "def apply_normal_fade_out(audio, transition, synth_audio_seg_samples, sr):\n",
        "    \"\"\"\n",
        "    This function applies the fade out operation on the `audio` array directly.\n",
        "    `synth_audio_seg_samples` is a tuple (start, stop), that contains the reference points to perform\n",
        "    fade out and fade in operations.\n",
        "    \"\"\"       \n",
        "    start, stop = synth_audio_seg_samples\n",
        "    \n",
        "    f_out_length_samples =  int(transition[0]['f_out_dur'] * sr)\n",
        "    \n",
        "    \n",
        "    # `stop_shrunk` refers to the new end point after silencing `time_gap` samples. \n",
        "    stop_shrunk = stop - int(transition[0]['time_gap'] * sr)\n",
        "    \n",
        "    # Set all the samples in the time gap to be 0.\n",
        "    audio[stop_shrunk:stop] = 0.0     \n",
        "    \n",
        "    #print(\"stop_shrunk: {}\".format(stop_shrunk))\n",
        "    \n",
        "    if transition[0]['f_out_curve'] == \"linear\":     \n",
        "        audio[stop_shrunk - f_out_length_samples:stop_shrunk] = audio[stop_shrunk - f_out_length_samples:stop_shrunk] * np.linspace(1.0, 0.0, num = f_out_length_samples)\n",
        "\n",
        "    elif transition[0]['f_out_curve'] == \"exp-concave\":\n",
        "        a = np.linspace(1.0, 0.0, num = f_out_length_samples)\n",
        "        x = transition[0]['exp_value']\n",
        "        fade_curve = a ** x\n",
        "        audio[stop_shrunk - f_out_length_samples:stop_shrunk] = audio[stop_shrunk - f_out_length_samples:stop_shrunk] * fade_curve\n",
        "        \n",
        "    elif transition[0]['f_out_curve'] == \"exp-convex\":\n",
        "        a = np.linspace(0.0, 1.0, num = f_out_length_samples)\n",
        "        x = transition[0]['exp_value']\n",
        "        fade_curve = 1 - a ** x\n",
        "        audio[stop_shrunk - f_out_length_samples:stop_shrunk] = audio[stop_shrunk - f_out_length_samples:stop_shrunk] * fade_curve\n",
        "       \n",
        "    elif transition[0]['f_out_curve'] == \"s-curve\":\n",
        "        n_1 = int(f_out_length_samples / 2)\n",
        "        a_1 = np.linspace(0, 1, num = n_1)\n",
        "        a_2 = np.linspace(0, 1, num = f_out_length_samples - n_1)\n",
        "        x = transition[0]['exp_value']\n",
        "        \n",
        "        convex = 0.5 * (1 - a_1 ** x) + 0.5\n",
        "        \n",
        "        concave = 0.5 * (1 - a_2)  ** x\n",
        "        \n",
        "        fade_curve = np.concatenate((convex, concave))\n",
        "        \n",
        "        audio[stop_shrunk - f_out_length_samples:stop_shrunk] = audio[stop_shrunk - f_out_length_samples:stop_shrunk] * fade_curve\n",
        "\n",
        "def apply_normal_fade_in(audio, transition, synth_audio_seg_samples, sr):\n",
        "    \"\"\"\n",
        "    This function applies the fade in operation on the `audio` array directly.\n",
        "    `synth_audio_seg_samples` is a tuple (start, stop), that contains the reference points to perform\n",
        "    fade out and fade in operations.    \n",
        "    \"\"\"\n",
        "    start, stop = synth_audio_seg_samples\n",
        "    f_in_length_samples =  int(transition[0]['f_in_dur'] * sr)\n",
        "    \n",
        "    if transition[0]['f_in_curve'] == \"linear\":        \n",
        "        audio[start:start + f_in_length_samples] = audio[start:start + f_in_length_samples] * np.linspace(0.0, 1.0, num = f_in_length_samples)      \n",
        "\n",
        "    elif transition[0]['f_in_curve'] == \"exp-concave\":\n",
        "        a = np.linspace(0.0, 1.0, num = f_in_length_samples)\n",
        "        x = transition[0]['exp_value']\n",
        "        fade_curve = a ** x\n",
        "        audio[start:start + f_in_length_samples] = audio[start:start + f_in_length_samples] * fade_curve\n",
        "        \n",
        "    elif transition[0]['f_in_curve'] == \"exp-convex\":\n",
        "        a = np.linspace(1.0, 0.0, num = f_in_length_samples)\n",
        "        x = transition[0]['exp_value']\n",
        "        fade_curve = 1 - a ** x\n",
        "        audio[start:start + f_in_length_samples] = audio[start:start + f_in_length_samples] * fade_curve\n",
        "        \n",
        "    elif transition[0]['f_in_curve'] == \"s-curve\":\n",
        "        n_1 = int(f_in_length_samples / 2)\n",
        "        a_1 = np.linspace(0, 1, num = n_1)\n",
        "        a_2 = np.linspace(0, 1, num = f_in_length_samples - n_1)\n",
        "        x = transition[0]['exp_value']\n",
        "        \n",
        "        concave = 0.5 * a_1 ** x\n",
        "        \n",
        "        convex = 0.5 * (1 - (1 - a_2)  ** x) + 0.5\n",
        "        \n",
        "        fade_curve = np.concatenate((concave, convex))\n",
        "        \n",
        "        audio[start:start + f_in_length_samples] = audio[start:start + f_in_length_samples] * fade_curve     \n",
        "\n",
        "def apply_cross_fade_out(audio, transition, sample, segment, synth_audio_seg_samples, sr):\n",
        "    \"\"\"\n",
        "    This function applies the fade out portion of the cross-fade operation on the `audio` array directly.\n",
        "    `sample` refers to the sample that is going to fade out.\n",
        "    `segments` refers to the segment boundaries in the original sound sample.\n",
        "    `synth_audio_seg_samples` is a tuple (start, stop), that contains the reference points \n",
        "    in the synthesised audio clip to perform fade out and fade in operations.      \n",
        "    \"\"\"\n",
        "    f_out_dur_samples = int(transition[0]['f_out_dur'] * sr)\n",
        "\n",
        "    if f_out_dur_samples > 0:\n",
        "      \n",
        "      start, stop = segment\n",
        "      start_sample = int(start * sr)\n",
        "      stop_sample = int(stop * sr)\n",
        "      \n",
        "      synth_audio_start, synth_audio_stop = synth_audio_seg_samples\n",
        "      \n",
        "      cf_out_audio, _ = sf.read(sample, start = stop_sample, stop = stop_sample + f_out_dur_samples)\n",
        "      cf_out_audio = librosa.util.normalize(cf_out_audio)\n",
        "      \n",
        "      \n",
        "      if transition[0]['f_out_curve'] == \"linear\":   \n",
        "          cf_out_audio = cf_out_audio * np.linspace(1.0, 0.0, num = f_out_dur_samples)\n",
        "          audio[synth_audio_stop:synth_audio_stop + f_out_dur_samples] = audio[synth_audio_stop:synth_audio_stop + f_out_dur_samples] + cf_out_audio\n",
        "\n",
        "      elif transition[0]['f_out_curve'] == \"exp-concave\":\n",
        "          a = np.linspace(1.0, 0.0, num = f_out_dur_samples)\n",
        "          x = transition[0]['exp_value']\n",
        "          fade_curve = a ** x\n",
        "          cf_out_audio = cf_out_audio * fade_curve\n",
        "          audio[synth_audio_stop:synth_audio_stop + f_out_dur_samples] = audio[synth_audio_stop:synth_audio_stop + f_out_dur_samples] + cf_out_audio\n",
        "          \n",
        "      elif transition[0]['f_out_curve'] == \"exp-convex\":\n",
        "          a = np.linspace(0.0, 1.0, num = f_out_dur_samples)\n",
        "          x = transition[0]['exp_value']\n",
        "          fade_curve = 1 - a ** x        \n",
        "          cf_out_audio = cf_out_audio * fade_curve\n",
        "          audio[synth_audio_stop:synth_audio_stop + f_out_dur_samples] = audio[synth_audio_stop:synth_audio_stop + f_out_dur_samples] + cf_out_audio\n",
        "\n",
        "      elif transition[0]['f_out_curve'] == \"s-curve\":\n",
        "          n_1 = int(f_out_dur_samples / 2)\n",
        "          a_1 = np.linspace(0, 1, num = n_1)\n",
        "          a_2 = np.linspace(0, 1, num = f_out_dur_samples - n_1)\n",
        "          x = transition[0]['exp_value']\n",
        "          \n",
        "          convex = 0.5 * (1 - a_1 ** x) + 0.5\n",
        "          \n",
        "          concave = 0.5 * (1 - a_2)  ** x\n",
        "          \n",
        "          fade_curve = np.concatenate((convex, concave))\n",
        "          \n",
        "          cf_out_audio = cf_out_audio * fade_curve\n",
        "          \n",
        "          audio[synth_audio_stop:synth_audio_stop + f_out_dur_samples] = audio[synth_audio_stop:synth_audio_stop + f_out_dur_samples] + cf_out_audio\n",
        "\n",
        "def apply_cross_fade_in(audio, transition, sample, segment, synth_audio_seg_samples, sr):\n",
        "    \"\"\"\n",
        "    This function applies the fade in portion of the cross-fade operation on the `audio` array directly.\n",
        "    `sample` refers to the sample that is going to fade in.\n",
        "    `segments` refers to the segment boundaries in the original sound sample.\n",
        "    `synth_audio_seg_samples` is a tuple (start, stop), that contains the reference points \n",
        "    in the synthesised audio clip to perform fade out and fade in operations.      \n",
        "    \"\"\"    \n",
        "    f_in_dur_samples = int(transition[0]['f_in_dur'] * sr)\n",
        "\n",
        "    if f_in_dur_samples > 0:\n",
        "    \n",
        "      start, stop = segment\n",
        "      start_sample = int(start * sr)\n",
        "      stop_sample = int(stop * sr)    \n",
        "      \n",
        "      synth_audio_start, synth_audio_stop = synth_audio_seg_samples\n",
        "      \n",
        "      cf_out_audio, _ = sf.read(sample, start = start_sample - f_in_dur_samples, stop = start_sample)\n",
        "      cf_out_audio = librosa.util.normalize(cf_out_audio)\n",
        "      \n",
        "\n",
        "      if transition[0]['f_in_curve'] == \"linear\":   \n",
        "          cf_out_audio = cf_out_audio * np.linspace(0.0, 1.0, num = f_in_dur_samples)    \n",
        "          audio[synth_audio_start - f_in_dur_samples:synth_audio_start] = audio[synth_audio_start - f_in_dur_samples:synth_audio_start] + cf_out_audio\n",
        "\n",
        "      elif transition[0]['f_in_curve'] == \"exp-concave\":\n",
        "          a = np.linspace(0.0, 1.0, num = f_in_dur_samples)\n",
        "          x = transition[0]['exp_value']\n",
        "          fade_curve = a ** x    \n",
        "          cf_out_audio = cf_out_audio * fade_curve    \n",
        "          audio[synth_audio_start - f_in_dur_samples:synth_audio_start] = audio[synth_audio_start - f_in_dur_samples:synth_audio_start] + cf_out_audio\n",
        "          \n",
        "      elif transition[0]['f_in_curve'] == \"exp-convex\":\n",
        "          a = np.linspace(1.0, 0.0, num = f_in_dur_samples)\n",
        "          x = transition[0]['exp_value']\n",
        "          fade_curve = 1 - a ** x  \n",
        "          cf_out_audio = cf_out_audio * fade_curve \n",
        "          audio[synth_audio_start - f_in_dur_samples:synth_audio_start] = audio[synth_audio_start - f_in_dur_samples:synth_audio_start] + cf_out_audio\n",
        "\n",
        "      elif transition[0]['f_in_curve'] == \"s-curve\":\n",
        "          n_1 = int(f_in_dur_samples / 2)\n",
        "          a_1 = np.linspace(0, 1, num = n_1)\n",
        "          a_2 = np.linspace(0, 1, num = f_in_dur_samples - n_1)\n",
        "          x = transition[0]['exp_value']\n",
        "          \n",
        "          concave = 0.5 * a_1 ** x\n",
        "          \n",
        "          convex = 0.5 * (1 - (1 - a_2)  ** x) + 0.5\n",
        "          \n",
        "          fade_curve = np.concatenate((concave, convex))\n",
        "          \n",
        "          cf_out_audio = cf_out_audio * fade_curve \n",
        "          \n",
        "          audio[synth_audio_start - f_in_dur_samples:synth_audio_start] = audio[synth_audio_start - f_in_dur_samples:synth_audio_start] + cf_out_audio      \n",
        "\n",
        "def create_audio_clip(audio_clip_length, transitions_list, samples_list, segments, sr):\n",
        "    \"\"\"\n",
        "    This function returns the synthesised audio clip after applying transitions.\n",
        "    \"\"\"\n",
        "    if (len(samples_list) < 1):\n",
        "        print(\"The samples_list argument is invalid!!\")\n",
        "        raise ValueError(\"The samples_list argument is invalid!!\")\n",
        "    \n",
        "    # Add the first audio segment.    \n",
        "    a, _ = sf.read(samples_list[0])\n",
        "    a = librosa.util.normalize(a)\n",
        "    \n",
        "    synth_audio = np.array([], dtype = np.float32)\n",
        "    # Add the first transition\n",
        "    if len(transitions_list) == 0:\n",
        "        # Trim the audio to the correct length\n",
        "        l_a = a.shape[0]\n",
        "        ss = int(audio_clip_length * sr)\n",
        "        if l_a == ss:\n",
        "          synth_audio = a[0:ss]\n",
        "        else:\n",
        "          l_st = np.random.randint(0, l_a - ss)\n",
        "          synth_audio = a[l_st:l_st + ss]\n",
        "       \n",
        "    elif len(transitions_list) > 0:\n",
        "        synth_audio, synth_audio_seg_samples = create_template_audio_clip(audio_clip_length, samples_list, segments, sr)\n",
        "        for i in range(len(transitions_list)):\n",
        "            if transitions_list[i][0]['type'] == \"normal\":\n",
        "                apply_normal_fade_out(synth_audio, transitions_list[i], synth_audio_seg_samples[i], sr)\n",
        "                apply_normal_fade_in(synth_audio, transitions_list[i], synth_audio_seg_samples[i + 1], sr)\n",
        "                \n",
        "            elif transitions_list[i][0]['type'] == \"cross-fade\":\n",
        "                apply_cross_fade_out(synth_audio, transitions_list[i], samples_list[i], segments[i], synth_audio_seg_samples[i], sr)\n",
        "                apply_cross_fade_in(synth_audio, transitions_list[i], samples_list[i + 1], segments[i + 1], synth_audio_seg_samples[i + 1], sr)\n",
        "        \n",
        "        # Trim the audio to the correct length\n",
        "        synth_audio = synth_audio[0:int(audio_clip_length * sr)]\n",
        "    #print(\"synth_audio {}\".format(synth_audio))\n",
        "    #sf.write(\"synth_audio.wav\", synth_audio, sr) \n",
        "    \n",
        "    return synth_audio\n",
        "\n",
        "def generate_multiclass_labels(audio_clip_length, transitions_list, class_list, sr = 22050.0, res = 220):\n",
        "  \"\"\"\n",
        "  This function generates labels.\n",
        "  `res` is in samples.\n",
        "  \"\"\"\n",
        "  res_t = 220 / sr\n",
        "  no_of_labels = int(np.ceil(audio_clip_length / res_t))\n",
        "\n",
        "  #t = np.linspace(start=0.0, stop=audio_clip_length, num=no_of_labels, endpoint=False)\n",
        "  #print(t)\n",
        "  #print(\"t shape is {}\".format(t.shape))\n",
        "\n",
        "\n",
        "\n",
        "  class_list_01 = [0 if i == \"speech\" else 1 if i == \"music\" else 2 for i in class_list]\n",
        "  class_list_01_opp = [1 if i == \"speech\" else 0 if i == \"music\" else 2 for i in class_list]\n",
        "  #print(\"class_list_01 is {}\".format(class_list_01))\n",
        "\n",
        "  labels = np.zeros((no_of_labels, 2), dtype = np.int16)\n",
        "  c = 0\n",
        "  prev_point = 0\n",
        "\n",
        "  for i in range(len(transitions_list)):\n",
        "    #print(i)\n",
        "    if class_list_01[c] != 2:\n",
        "      labels[prev_point:int(transitions_list[i][1] / res_t), class_list_01[c]] = 1\n",
        "    # Convert the fades into multiclass points.\n",
        "    # The previous fade out should only be considered if it is not the first sample.\n",
        "    if (True):\n",
        "      if (transitions_list[i][0]['type'] == \"cross-fade\"):\n",
        "        end_sample = int(transitions_list[i][1] / res_t) + int(transitions_list[i][0]['f_out_dur'] / res_t)\n",
        "        start_sample = int(transitions_list[i][1] / res_t)\n",
        "        #print(\"start_sample is {}\".format(start_sample))\n",
        "        #print(\"end_sample is {}\".format(end_sample))\n",
        "        if class_list_01[c] != 2:\n",
        "          labels[start_sample:end_sample, class_list_01[c]] = 1\n",
        "\n",
        "        start_sample = int(transitions_list[i][1] / res_t) - int(transitions_list[i][0]['f_in_dur'] / res_t)\n",
        "        end_sample = int(transitions_list[i][1] / res_t)\n",
        "        #print(\"start_sample is {}\".format(start_sample))\n",
        "        #print(\"end_sample is {}\".format(end_sample))\n",
        "        if class_list_01[c + 1] != 2:\n",
        "          labels[start_sample:end_sample, class_list_01[c + 1]] = 1\n",
        "\n",
        "      elif (transitions_list[i][0]['type'] == \"normal\"):\n",
        "        start_sample = int(transitions_list[i][1] / res_t) - int(transitions_list[i][0]['time_gap'] / res_t)\n",
        "        end_sample = int(transitions_list[i][1] / res_t)\n",
        "        if class_list_01[c] != 2:\n",
        "          labels[start_sample:end_sample, class_list_01[c]] = 0\n",
        "\n",
        "      else:\n",
        "        print(\"Hmmm... Something is wrong in the type of transitions...\")\n",
        "\n",
        "\n",
        "    # if (i != len(transitions_list) - 1):\n",
        "    #   end_sample = int(transitions_list[i][1] / res_t) + int(transitions_list[i]['fade_in_dur'] / res_t)\n",
        "    #   start_sample = int(transitions_list[i][1] / res_t)\n",
        "    #   labels[start_sample:end_sample][class_list_01_opp[c]] = 1\n",
        "\n",
        "    prev_point = int(transitions_list[i][1] / res_t)\n",
        "    c += 1\n",
        "\n",
        "  if class_list_01[c] != 2:\n",
        "    labels[prev_point:no_of_labels,class_list_01[c]] = 1\n",
        "  \n",
        "  return labels\n",
        "\n",
        "\n",
        "def generate_sed_eval_labels(audio_clip_length, transitions_list, class_list, sr = 22050.0, res = 220):\n",
        "  prev_point = 0.0\n",
        "\n",
        "  labels = []\n",
        "  c = 0\n",
        "\n",
        "  for i in range(len(transitions_list)):\n",
        "    if (transitions_list[i][0]['type'] == \"cross-fade\"):\n",
        "      end_point = transitions_list[i][1]\n",
        "      end_point += transitions_list[i][0]['f_out_dur']\n",
        "      labels.append((prev_point, end_point, class_list[c]))\n",
        "\n",
        "    elif (transitions_list[i][0]['type'] == \"normal\"):\n",
        "      end_point = transitions_list[i][1]\n",
        "      end_point -= transitions_list[i][0]['time_gap']\n",
        "      labels.append((prev_point, end_point, class_list[c]))\n",
        "\n",
        "    else:\n",
        "        print(\"Hmmm... Something is wrong in the type of transitions...\")\n",
        "\n",
        "    prev_point = transitions_list[i][1]\n",
        "    c += 1\n",
        "\n",
        "  labels.append((prev_point, audio_clip_length, class_list[c]))\n",
        "\n",
        "  return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GacKgSvO0UqI"
      },
      "source": [
        "## Combining the data syntheses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmFTJZAb0wJK"
      },
      "source": [
        "def synthesise_combined_audio_examples(no_of_examples, mel_dir, sr = 22050, audio_clip_length = 8.0, batch_size = 1, offset = 0):\n",
        "    \"\"\"\n",
        "    This function synthesised audio examples and stores them in a directory.\n",
        "    \"\"\"           \n",
        "\n",
        "    count_old = 0\n",
        "    count_mixed = 0\n",
        "\n",
        "    no_of_batches = int(np.floor(no_of_examples / batch_size))\n",
        "\n",
        "    for b in range(no_of_batches):\n",
        "\n",
        "      l = int(audio_clip_length * sr)\n",
        "\n",
        "      res_t = 220 / sr\n",
        "      no_of_labels = int(np.ceil(audio_clip_length / res_t))\n",
        "      labels = np.zeros((no_of_labels, 2), dtype=np.int16)\n",
        "\n",
        "      c = random.choice([\"old\", \"mixed\"])\n",
        "      #c = \"old\"\n",
        "\n",
        "      if c == \"mixed\":\n",
        "        count_mixed += 1\n",
        "        synth_audio, transition = create_mixed_audio_clip()\n",
        "        synth_audio = librosa.util.normalize(synth_audio)\n",
        "        \n",
        "        labels[:, 0:2] = generate_mixed_multiclass_labels(transition)\n",
        "\n",
        "      elif c == \"old\":\n",
        "        count_old += 1\n",
        "        p = create_random_transition_points(audio_clip_length, 1.0) # Create random transition points.\n",
        "        \n",
        "        transitions_list = create_transition_list(p) # Create a list of transitions.\n",
        "        \n",
        "        class_list = create_class_list(len(p) + 1) # Create a list of alternating classes for the segments.\n",
        "\n",
        "                    \n",
        "        samples_list = create_samples_list(class_list, music_sounds, speech_sounds, noise_sounds) # Create a list of randomly selected sounf files from the `music_sounds` and `speech_sounds` folder\n",
        "        \n",
        "        segment_lengths = get_segment_lengths(transitions_list, audio_clip_length)\n",
        "        \n",
        "        random_segments = get_random_segments(samples_list, segment_lengths)\n",
        "        \n",
        "        synth_audio = create_audio_clip(audio_clip_length, transitions_list, samples_list, random_segments, 22050)\n",
        "        synth_audio = librosa.util.normalize(synth_audio)           \n",
        "        \n",
        "        labels[:, 0:2] = generate_multiclass_labels(audio_clip_length, transitions_list, class_list)\n",
        "\n",
        "      else:\n",
        "        print(\"\\n\\n\\n Uncountered unexpected choice between old and mixed!!!! \\n\\n\\n\")\n",
        "\n",
        "\n",
        "          #np.save(n_DbS_label, DbS_label)\n",
        "      # multi_to_cat(labels)\n",
        "      mel_id = 'mel-id-' + str(b + 1 + offset)\n",
        "\n",
        "      pp = mel_dir + '/block-id-' + str(get_block_id(mel_id)) + '/'\n",
        "\n",
        "      if not os.path.isdir(pp):\n",
        "        os.mkdir(pp)\n",
        "\n",
        "      #synth_audio = synth_audio.astype(np.float32)\n",
        "      M = get_log_melspectrogram(synth_audio)\n",
        "\n",
        "      np.save(pp + 'mel-id-' + str(b + 1 + offset) + '.npy', M.T)\n",
        "      np.save(pp + 'mel-id-label-' + str(b + 1 + offset) + '.npy', labels)\n",
        "\n",
        "    return count_old, count_mixed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctI9dSqPBtmc"
      },
      "source": [
        "def synthesise_examples_OF(music_dir, speech_dir, mel_dir, sr = 22050, audio_clip_length = 8.0, batch_size = 1, offset = 0):\n",
        "  \"\"\"\n",
        "  This function synthesised audio examples and stores them in a directory.\n",
        "  \"\"\"           \n",
        "  example_len = int(sr * audio_clip_length)\n",
        "\n",
        "  batch_size = 1\n",
        "  no_of_labels = 802\n",
        "\n",
        "  b = 0\n",
        "\n",
        "  for s in speech_sounds:\n",
        "    audio, _ = sf.read(s)\n",
        "    audio_len = audio.shape[0]\n",
        "    if audio_len < example_len:\n",
        "      continue\n",
        "    else:\n",
        "      no_of_steps = int(audio_len // example_len)\n",
        "      i = 0\n",
        "      for i in range(no_of_steps):\n",
        "        synth_audio = audio[example_len*i:example_len*(i + 1)]\n",
        "        batch_audio = np.reshape(synth_audio, (1, example_len))\n",
        "        synth_audio = librosa.util.normalize(synth_audio)\n",
        "        labels = np.zeros((no_of_labels, 2), dtype = np.int16)\n",
        "        labels[:, 0] = 1\n",
        "          \n",
        "        mel_id = 'mel-id-' + str(b + 1 + offset)\n",
        "        M = get_log_melspectrogram(synth_audio)\n",
        "\n",
        "        pp = mel_dir + '/block-id-' + str(get_block_id(mel_id)) + '/'\n",
        "\n",
        "        if not os.path.isdir(pp):\n",
        "          os.mkdir(pp)\n",
        "\n",
        "        np.save(pp + 'mel-id-' + str(b + 1 + offset) + '.npy', M.T)\n",
        "        np.save(pp + 'mel-id-label-' + str(b + 1 + offset) + '.npy', labels)\n",
        "        \n",
        "        b += 1\n",
        "\n",
        "      if audio_len >= no_of_steps * example_len:\n",
        "        synth_audio = audio[audio_len - example_len:audio_len]\n",
        "        synth_audio = librosa.util.normalize(synth_audio)\n",
        "\n",
        "        labels = np.zeros((no_of_labels, 2), dtype = np.int16)\n",
        "        labels[:, 0] = 1\n",
        "\n",
        "        mel_id = 'mel-id-' + str(b + 1 + offset)\n",
        "        M = get_log_melspectrogram(synth_audio)\n",
        "\n",
        "        pp = mel_dir + '/block-id-' + str(get_block_id(mel_id)) + '/'\n",
        "\n",
        "        if not os.path.isdir(pp):\n",
        "          os.mkdir(pp)\n",
        "\n",
        "        np.save(pp + 'mel-id-' + str(b + 1 + offset) + '.npy', M.T)\n",
        "        np.save(pp + 'mel-id-label-' + str(b + 1 + offset) + '.npy', labels)\n",
        "        \n",
        "        b += 1\n",
        "\n",
        "  for s in music_sounds:\n",
        "    audio, _ = sf.read(s)\n",
        "    audio_len = audio.shape[0]\n",
        "    if audio_len < example_len:\n",
        "      continue\n",
        "    else:\n",
        "      no_of_steps = int(audio_len // example_len)\n",
        "      i = 0\n",
        "      for i in range(no_of_steps):\n",
        "        synth_audio = audio[example_len * i:example_len * (i + 1)]\n",
        "        synth_audio = librosa.util.normalize(synth_audio)\n",
        "\n",
        "        labels = np.zeros((no_of_labels, 2))\n",
        "        labels[:, 1] = 1\n",
        "\n",
        "          \n",
        "        mel_id = 'mel-id-' + str(b + 1 + offset)\n",
        "        M = get_log_melspectrogram(synth_audio)\n",
        "\n",
        "        pp = mel_dir + '/block-id-' + str(get_block_id(mel_id)) + '/'\n",
        "\n",
        "        if not os.path.isdir(pp):\n",
        "          os.mkdir(pp)\n",
        "\n",
        "        np.save(pp + 'mel-id-' + str(b + 1 + offset) + '.npy', M.T)\n",
        "        np.save(pp + 'mel-id-label-' + str(b + 1 + offset) + '.npy', labels)\n",
        "        \n",
        "        b += 1\n",
        "\n",
        "      if audio_len >= no_of_steps * example_len:\n",
        "        synth_audio = audio[audio_len - example_len:audio_len]\n",
        "        synth_audio = librosa.util.normalize(synth_audio)\n",
        "        labels = np.zeros((no_of_labels, 2), dtype = np.int16)\n",
        "        batch_labels[:, 1] = 1\n",
        "\n",
        "          \n",
        "        mel_id = 'mel-id-' + str(b + 1 + offset)\n",
        "        labels = get_log_melspectrogram(synth_audio)\n",
        "\n",
        "        pp = mel_dir + '/block-id-' + str(get_block_id(mel_id)) + '/'\n",
        "\n",
        "        if not os.path.isdir(pp):\n",
        "          os.mkdir(pp)\n",
        "\n",
        "        np.save(pp + 'mel-id-' + str(b + 1 + offset) + '.npy', M.T)\n",
        "        np.save(pp + 'mel-id-label-' + str(b + 1 + offset) + '.npy', labels)\n",
        "        \n",
        "        b += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_AOoP1wH6wf"
      },
      "source": [
        "\n",
        "speech_sounds = speech_list\n",
        "music_sounds = music_list\n",
        "noise_sounds = noise_list\n",
        "print(len(speech_sounds))\n",
        "print(len(music_sounds))\n",
        "print(len(noise_sounds))\n",
        "\n",
        "# Location of the directory to store the audio examples.\n",
        "\n",
        "mel_dir = \"/content/Mel Files\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp8tiMJXyePo"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "from zipfile import ZipFile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpD_XiArV9wY"
      },
      "source": [
        "synth_dir = \"/content/drive/My Drive/Data Synthesis\"\n",
        "try: \n",
        "  os.makedirs(synth_dir, exist_ok = True) \n",
        "  print(\"Directory '%s' created successfully\" %raw_dir) \n",
        "except OSError as error: \n",
        "    print(\"Directory '%s' can not be created\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RmuWJf0mdEQ"
      },
      "source": [
        "\"\"\"\n",
        "Split the data synthesis into parts of 5120 examples. \n",
        "In the below code, i ranges from 0 to 8, and thus creating 5120 * 8 = 40960 examples.\n",
        "If you are synthesising the val set, a range from 0 to 1 might be appropriate.\n",
        "\"\"\"\n",
        "for i in range(0, 8):\n",
        "  os.mkdir('/content/Mel Files')\n",
        "  count_old, count_mixed = synthesise_combined_audio_examples(5120, mel_dir, offset = 5120 * i)\n",
        "  mel_files = glob.glob('/content/Mel Files/**/*.npy', recursive=True)\n",
        "  print(mel_files)\n",
        "  zip_file_name = synth_dir + \"/Train - d_\" + str(i + 1) + \".zip\"\n",
        "\n",
        "  with ZipFile(zip_file_name, 'w') as my_zip:\n",
        "    for f in mel_files:\n",
        "      my_zip.write(f)\n",
        "\n",
        "  shutil.rmtree('/content/Mel Files')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}